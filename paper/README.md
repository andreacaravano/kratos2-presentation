Artifact for Kratos2: an SMT-Based Model Checker for Imperative Programs
=========================================================================

# Purpose of the artifact

The artifact aims for reproducing experimental evaluation of the paper with
tools CPAchecker and Kratos2. The artifact also provides examples of using
Kratos2 manually.

The artifact **does not** reproduce the experimental results of VeriAbs, as its
license only grants a personal and non-transferrable right to use VeriAbs to run
the evaluation on SV-COMP benchmarks and publish the results. However, the
license does not permit its other use, copying or modifying, enhancing or
creating derivative works, and redistribution of VeriAbs.

VeriAbs is **not required** to execute the experiments and it is **not
expected**. However, if you want to perform the experimental evaluation with
VeriAbs and if you agree with its license, you can download the tool on your
own. Simply download it from the SV-COMP 2022 archives from
https://gitlab.com/sosy-lab/sv-comp/archives-2022/raw/svcomp22/2022/veriabs.zip
and unzip the archive into the "~/artifact/tools" directory (i.e., you will get
a directory "~/artifact/tools/VeriAbs"). The scripts that execute and analyze
experiments will recognize and use VeriAbs. We note that in this case, it is
your responsibility to comply with the license of VeriAbs.

# HW requirements

The artifact does not have any specific HW requirements and should work on any
reasonable personal computer with at least 1 CPU core and at least 8 GiB of RAM.
For testing, we used computers with Intel Core i7-8700 CPU @ 3.20 GHz and 32 GiB
of RAM and allocated 1 CPU core and 16 GiB of RAM to the virtual machine. If you
allocate more CPU cores and memory, you can run the experiments in parallel and
decrease the wall-time necessary to run the experiments.

# Setting up

Just import the VM image `Kratos2-CAV23.ova` into VirtualBox and start it. We
recommend assigning at least 1 CPU core and 16 GiB of RAM to the virtual
machine, if possible.

Both the username and the password to the virtual machine are "ae".

# Contents of the artifact

The artifact contains a single virtual machine with all required tools and
scripts.

The structure of the files contained in the virtual machine is the following:
- `artifact/benchmarks` contains the full checkout of SV-COMP benchmark
  repository and also our randomly selected subset of the benchmarks;
- `artifact/docs` contains a snapshot of the documentation of Kratos2;
- `artifact/examples` contains some simple K2 and C programs that can be used
  to try Kratos2 manually;
- `artifacts/experiments` contains configurations for the tool BenchExec to run
  the experimental evaluation;
- `artifacts/outputs` is used to store the processed experimental results and
  the corresponding tables and plots;
- `artifacts/results` is used to store the unprocessed experimental results
  generated by BenchExec. The directory also contains our experimental results
  that were used in the paper in the subdirectory
  `artifact/results/results-paper`;
- `artifacts/scripts` contains the scripts that are used for the experimental
  evaluation and for analysis of the results;
- `artifacts/tools` contains the compared tools CPAchecker and Kratos2;
- `artifacts/vm-preparation` documents the steps took to create the virtual
  machine.

Some of the directories contain their own README, which document the contents in
more detail.

All the following commands suppose that the working directory is `~/artifact`.

# Reproducing experimental evaluation of the paper

## TL;DR

If you want to reproduce the results of our experimental evaluation:

1) Run one of the following commands:
   - `bash scripts/run-all.sh`, if you want to reproduce the entire
     experimental evaluation of the paper with timeout 900 s (this takes about
     **40 days** of CPU time without VeriAbs and 53 days with VeriAbs),
   - `bash scripts/run-subset.sh 90` (recommended for evaluation within 8
     hours), if you want to reproduce the experimental evaluation on a small
     subset of benchmarks with timeout 90 s (this takes about **6 hours** of CPU
     time without VeriAbs and 9 hours with VeriAbs),
   - `bash scripts/run-subset.sh 5` (recommended for the smoke test), if you
     want to reproduce the experimental evaluation on a small subset of
     benchmarks with timeout 5 s (this takes about **30 minutes** of CPU time
     without VeriAbs and 50 minutes with VeriAbs),
   - or do nothing, if you just want to generate tables and plots from our
     attached experimental results that were used in the paper.

2) Run one of the following commands:
   - `python scripts/generate_csvs.py results/results-all/`, if you executed the
     experiments on all benchmarks,
   - `python scripts/generate_csvs.py results/results-subset/`, if you executed the
     experiments on the small subset of benchmarks,
   - or `python scripts/generate_csvs.py results/results-paper/`, if you just
     want to generate tables and plots from our attached experimental results
     that which were used in the paper.

3) Run `bash scripts/analyze_results.sh`.

4) View the PDF file `outputs/results.pdf`.

## Running experiments

We provide two versions of the benchmarks: the full set of 5400 SV-COMP
benchmarks and a smaller randomly selected subset of 193 benchmarks, which is
useful for evaluation with limited resources (i.e., CPU cores, RAM, and time
available for the evaluation).

Both benchmark sets are executed using BenchExec and can be executed by the
following scripts
- `bash scripts/run-all.sh` or
- `bash scripts/run-subset.sh`.

Both of the scripts take an optional first argument that sets the time-out in
seconds. The default time-out is 900 seconds, which is used in the paper.

Both of the scripts also take an optional second argument with the number of
runs that are executed in parallel. You can use this if you assigned more CPU
cores to the virtual machine and you also assigned more memory (each run
requires 8 GB of memory). The default number of parallel runs is 1.

Each run produces compressed XML files with the results, as generated by
BenchExec. The results are stored in `results/results-all` or
`results/results-subset`, depending on the script used. The results in each
directory are overwritten by the next execution of the same script (e.g., when
you execute the script with a different timeout).

If you want to examine the definitions of the experiments, i.e., arguments of
the tools, resource limits, and used benchmarks, the BenchExec configuration
files are:
- `experiments/benchmark-cpachecker-all.xml`
- `experiments/benchmark-kratos-all.xml`
- `experiments/benchmark-veriabs-all.xml`
- `experiments/benchmark-cpachecker-subset.xml`
- `experiments/benchmark-kratos-subset.xml`
- `experiments/benchmark-veriabs-subset.xml`

Out of the box, the directory `results/results-paper` contains our experimental
results that are used in the paper. These can also be used for the further
analysis.

*Important:* Note that if you performed the evaluation only on the subset of
benchmarks and with a smaller timeout than 900 seconds used in the paper, the
numbers of solved benchmarks and the relative performance of the solvers can
*substantially* differ from the results in the paper. This is expected, as the
tools use sequential portfolio approach of multiple analyses with predefined
timeouts and if you decrease the global timeout, some of the analyses are not
executed. However, even the evaluation on the subset of bechmarks demonstrates
that Kratos2 works and is able to solve a substantial number of benchmarks from
SV-COMP repository; in some cases significantly faster than CPAchecker and VeriAbs.

## Analyzing the results

The XML files generated by running the experiments can be analyzed further in
two steps.

First, the script `python scripts/generate_csvs.py DIR_WITH_RESULTS` processes
the generated XML files using the tool `table-generator` from BenchExec
distribution and produces a list of CSV files
`outputs/csv/{CATEGORY}.table.csv`. For each benchmark family, the file contains
rows corresponding to the benchmarks of the family and columns corresponding to
the individual solvers (i.e., `cpachecker`, `kratos2`, or `veriabs`) and their
result, used CPU-time, wall-time, and memory.

Consequently, the generated CSV files can be used to produce tables and plots
similar to those that are used in the paper. These are generated in two steps,
which are performed internally by the script `scripts/analyze_results.sh` for
convenience.

1) The script runs the R script `Rscript scripts/analyze_results.R`, which loads
   the CSV files, preprocesses and analyzes the data and produces the following
   tables and plots:
   - `outputs/tables/solved.tex`
   - `outputs/tables/detailed.tex`
   - `outputs/tables/unsupported.tex`
   - `outputs/tables/unsupported_family.tex`
   - `outputs/tables/incorrect.tex`
   - `outputs/tables/unique_family.tex`
   - `outputs/tables/unique_total.tex`
   - `outputs/figures/quantile.pdf`
   - `outputs/figures/quantile_log.pdf`
   - `outputs/figures/quantile_families.pdf`

2) The script runs `pdflatex` on the file `outputs/results.tex`, which includes
   all the tables and plots, and produces a single PDF file `outputs/results.pdf`.

Note that the script `scripts/analyze_results.sh` uses the CSV files that you
generated in the first step. You can therefore run

```
python scripts/generate_csvs.py DIR_WITH_RESULTS
bash scripts/analyze_results.sh
```

repeatedly with different `DIR_WITH_RESULTS` to produce tables and plots for
different experimental results (your run on all benchmarks, your run on subset
of benchmarks, or our provided results).


# Using Kratos2 and c2kratos

If you want to run Kratos2 manually, you can use the following examples.

## Running on K2 programs

The directory `examples/k2` contains several programs in K2 language, which you
can use to test Kratos2. Moreover, the directory `docs` contains the file
`docs/K2-language.md`, which describes the language K2, and the file
`docs/running-kratos.md`, which describes how to run Kratos2.

If you want to run Kratos2 on programs in K2 language, just run
```
tools/kratos2/bin/kratos examples/k2/testprog.k2
```
and you get the transition system representation of the input program. If you
also want perform verification of the program, run
```
tools/kratos2/bin/kratos -stage=mc examples/k2/testprog.k2
```
or
```
tools/kratos2/bin/kratos -stage=symexec -error_id=ERROR examples/k2/testprog.k2
```
to run model checking or symbolic execution, respectively.

To translate the K2 program into another representation, run for example
```
tools/kratos2/bin/kratos -stage=trans -trans_output_format=nuxmv examples/k2/testprog.k2
```
to translate the program into a transition system represented in nuXmv language.

To see other formats, verification back-ends, and other features of Kratos2, run
```
tools/kratos2/bin/kratos -h
```

## Running on C programs

You can translate C programs to K2 using `tools/kratos2/tools/c2kratos.py`. To
translate the example C program from the appendix of the paper, run
```
python tools/kratos2/tools/c2kratos.py examples/c/simple.c
```

You can also use `c2kratos.py` to translate SV-COMP programs based on the
corresponding specification file. This is the recommended usage, as it uses
settings compatible with SV-COMP rules (for example the standard libraries and
bit-widths of standard data). For example
```
python tools/kratos2/tools/c2kratos.py --svcomp --svcomp-spec benchmarks/sv-benchmarks/c/properties/unreach-call.prp benchmarks/sv-benchmarks/c/bitvector/byte_add_1-1.c
```
translates the SV-COMP benchmark `bitvector/byte_add_1-1.c` with the target property
`unreach-call`.

You can either save the result into a file, or pipe the resulting K2 code
directly into Kratos2. For example, you can run
```
python tools/kratos2/tools/c2kratos.py --svcomp --svcomp-spec benchmarks/sv-benchmarks/c/properties/unreach-call.prp benchmarks/sv-benchmarks/c/bitvector/byte_add_1-1.c | tools/kratos2/bin/kratos -stage=mc
```
to model check the above program (takes around 10 seconds).

To see other options of the translation, run
```
python tools/kratos2/tools/c2kratos.py -h
```

Alternatively, you can use the provided script `scripts/kratos_svcomp.py`, which
translates the C program to K2 and then runs sequential portfolio of different
model checking back-ends of Kratos2:
```
python scripts/kratos_svcomp.py benchmarks/sv-benchmarks/c/bitvector/byte_add_1-1.c
```